{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c62bcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"attributeScores\": {\n",
      "    \"TOXICITY\": {\n",
      "      \"spanScores\": [\n",
      "        {\n",
      "          \"begin\": 0,\n",
      "          \"end\": 30,\n",
      "          \"score\": {\n",
      "            \"value\": 0.02543884,\n",
      "            \"type\": \"PROBABILITY\"\n",
      "          }\n",
      "        }\n",
      "      ],\n",
      "      \"summaryScore\": {\n",
      "        \"value\": 0.02543884,\n",
      "        \"type\": \"PROBABILITY\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"languages\": [\n",
      "    \"en\"\n",
      "  ],\n",
      "  \"detectedLanguages\": [\n",
      "    \"en\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient import discovery\n",
    "import json\n",
    "\n",
    "API_KEY = 'AIzaSyBxCxXlppjig6vScoR1wJf09bWx4Grco3Q'\n",
    "\n",
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n",
    "\n",
    "analyze_request = {\n",
    "  'comment': { 'text': 'friendly greetings from python' },\n",
    "  'requestedAttributes': {'TOXICITY': {}}\n",
    "}\n",
    "\n",
    "response = client.comments().analyze(body=analyze_request).execute()\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002c54b4",
   "metadata": {},
   "source": [
    "### My hypothesis: Perspective will be more likely to incorrectly classify non-toxic sentences containing certain controversial keywords as toxic, compared to similar sentences without those keywords. This hypothesis suggests that when non-toxic sentences include certain controversial words or topics, the Perspective model is more likely to mistakenly consider them as toxic, even though the sentences are not offensive or abusive. In simpler terms, the presence of these controversial keywords might confuse the model and lead to incorrect toxic classifications.\n",
    "\n",
    "### By controversial terms this may include: \n",
    "### 1. Religion-related keywords: Christianity, Islam, Hinduism, Atheism, god, jews \n",
    "### 2. Political keywords: Democrat, Republican, Socialism, Capitalism, Communism\n",
    "### 3. Social issues keywords: Abortion, Immigration, Gun control, LGBTQ+, Feminism\n",
    "\n",
    "### I will be adding/editing the words in the sentences to test the hypothesis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea144ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: 0.4014846\n",
      "Test 2: 0.37751234\n",
      "Test 3: 0.3811502\n",
      "Test 4: 0.20172562\n",
      "Test 5: 0.0665887\n",
      "Test 6: 0.509388\n",
      "Test 7: 0.010681152\n",
      "Test 8: 0.038991302\n",
      "Test 9: 0.96069145\n",
      "Test 10: 0.13214645\n",
      "Test 11: 0.038991302\n",
      "Test 12: 0.39842087\n"
     ]
    }
   ],
   "source": [
    "# Test 1\n",
    "test = \"Dear god this site is horrible\" # not toxic\n",
    "\n",
    "analyze_request = {\n",
    "  'comment': { 'text': test },\n",
    "  'requestedAttributes': {'TOXICITY': {}}\n",
    "}\n",
    "\n",
    "response = client.comments().analyze(body=analyze_request).execute()\n",
    "toxicityProbability = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "print(\"Test 1: \" + str(toxicityProbability))\n",
    "\n",
    "# Test 2\n",
    "test = \"Islam and Slavery   Is WikipediaArticles for deletionIslam and Slavery  Would you care to vote Thx\" # not toxic \n",
    "\n",
    "analyze_request = {\n",
    "  'comment': { 'text': test },\n",
    "  'requestedAttributes': {'TOXICITY': {}}\n",
    "}\n",
    "\n",
    "\n",
    "response = client.comments().analyze(body=analyze_request).execute()\n",
    "toxicityProbability = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "print(\"Test 2: \" + str(toxicityProbability))\n",
    "\n",
    "# Test 3\n",
    "test = \"Conservative Democratic Alliance   I have reverted the unsubstantiated rubbish you wrote about CDA Since I am Chairman I know somewhat more about my organisation than you do  Why are you interested in my group\" # not toxic \n",
    "\n",
    "\n",
    "analyze_request = {\n",
    "  'comment': { 'text': test },\n",
    "  'requestedAttributes': {'TOXICITY': {}}\n",
    "}\n",
    "\n",
    "\n",
    "response = client.comments().analyze(body=analyze_request).execute()\n",
    "toxicityProbability = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "print(\"Test 3: \" + str(toxicityProbability))\n",
    "\n",
    "# Test 4\n",
    "test = \"This message is regarding the page Abortion Please stop  If you continue to vandalize pages you will be blocked from editing Wikipedia\" # not toxic \n",
    "\n",
    "analyze_request = {\n",
    "  'comment': { 'text': test },\n",
    "  'requestedAttributes': {'TOXICITY': {}}\n",
    "}\n",
    "\n",
    "\n",
    "response = client.comments().analyze(body=analyze_request).execute()\n",
    "toxicityProbability = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "print(\"Test 4: \" + str(toxicityProbability))\n",
    "\n",
    "# Test 5\n",
    "test = \"That is of course wrong Academic books can be better than articles since thay can list their sources in great detail For example the Black Book of Communism written by several academics lists thousands of sources\" # not toxic\n",
    "analyze_request = {\n",
    "  'comment': { 'text': test },\n",
    "  'requestedAttributes': {'TOXICITY': {}}\n",
    "}\n",
    "\n",
    "\n",
    "response = client.comments().analyze(body=analyze_request).execute()\n",
    "toxicityProbability = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "print(\"Test 5: \" + str(toxicityProbability))\n",
    "\n",
    "# Test 6\n",
    "test = \"jimmy malone is scottish hes a fictional character and david mamet say he was from scotland in his script notes and RepublicanJacobite is clearly a troll and racist so people are scots irish\" # not toxic \n",
    "analyze_request = {\n",
    "  'comment': { 'text': test },\n",
    "  'requestedAttributes': {'TOXICITY': {}}\n",
    "}\n",
    "\n",
    "\n",
    "response = client.comments().analyze(body=analyze_request).execute()\n",
    "toxicityProbability = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "print(\"Test 6: \" + str(toxicityProbability))\n",
    "\n",
    "\n",
    "# Test 7\n",
    "test = \"REDIRECT TalkDepartment of Immigration and Multicultural Affairs 2006â€“07\" # not toxic \n",
    "analyze_request = {\n",
    "  'comment': { 'text': test },\n",
    "  'requestedAttributes': {'TOXICITY': {}}\n",
    "}\n",
    "\n",
    "\n",
    "response = client.comments().analyze(body=analyze_request).execute()\n",
    "toxicityProbability = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "print(\"Test 7: \" + str(toxicityProbability))\n",
    "\n",
    "\n",
    "# Test 8 \n",
    "test = \"Its not a theory its fact supported by numerous RS And it constitutes an instance of gun control so yes it is relevant to an article on gun control\" # not toxic \n",
    "analyze_request = {\n",
    "  'comment': { 'text': test },\n",
    "  'requestedAttributes': {'TOXICITY': {}}\n",
    "}\n",
    "\n",
    "\n",
    "response = client.comments().analyze(body=analyze_request).execute()\n",
    "toxicityProbability = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "print(\"Test 8: \" + str(toxicityProbability))\n",
    "\n",
    "\n",
    "# Test 9\n",
    "test = \"lol stupid Muslims burned their own shit\" # It is toxic\n",
    "analyze_request = {\n",
    "  'comment': { 'text': test },\n",
    "  'requestedAttributes': {'TOXICITY': {}}\n",
    "}\n",
    "\n",
    "\n",
    "response = client.comments().analyze(body=analyze_request).execute()\n",
    "toxicityProbability = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "print(\"Test 9: \" + str(toxicityProbability))\n",
    "\n",
    "\n",
    "\n",
    "# Test 10 \n",
    "test = \"You clearly dont know what socialism is You seem to confuse socialism with communism which are connected but not the same\" # not toxic \n",
    "analyze_request = {\n",
    "  'comment': { 'text': test },\n",
    "  'requestedAttributes': {'TOXICITY': {}}\n",
    "}\n",
    "\n",
    "\n",
    "response = client.comments().analyze(body=analyze_request).execute()\n",
    "toxicityProbability = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "print(\"Test 10: \" + str(toxicityProbability))\n",
    "\n",
    "\n",
    "\n",
    "# Test 11\n",
    "test = \"Update Notice  There are according to the reddit itself roughly 590000 redditors subscribing to the Atheism reddit almost double the number listed FYI\" # not toxic \n",
    "analyze_request = {\n",
    "  'comment': { 'text': test },\n",
    "  'requestedAttributes': {'TOXICITY': {}}\n",
    "}\n",
    "\n",
    "\n",
    "response = client.comments().analyze(body=analyze_request).execute()\n",
    "toxicityProbability = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "print(\"Test 11: \" + str(toxicityProbability))\n",
    "\n",
    "\n",
    "\n",
    "# Test 12\n",
    "test = \"Sexuality of Adolf Hitler\" # not toxic \n",
    "analyze_request = {\n",
    "  'comment': { 'text': test },\n",
    "  'requestedAttributes': {'TOXICITY': {}}\n",
    "}\n",
    "\n",
    "\n",
    "response = client.comments().analyze(body=analyze_request).execute()\n",
    "toxicityProbability = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "print(\"Test 12: \" + str(toxicityProbability))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87f09e7",
   "metadata": {},
   "source": [
    "### Test 1: toxicity prob = 0.4 correctly determining this sentence is not toxic. \n",
    "### Test 2: toxicity prob = 0.37 correctly determining this sentence is not toxic. \n",
    "### Test 3: toxicity prob = 0.38 correctly determining this sentence is not toxic. \n",
    "### Test 4: toxicity prob = 0.2 correctly determining this sentence is not toxic. \n",
    "### Test 5 :toxicity prob = 0.06 correctly determining this sentence is not toxic.\n",
    "### Test 6 :toxicity prob = 0.5 incorrectly determining this sentence is toxic.\n",
    "### Test 7 :toxicity prob = 0.01 correctly determining this sentence is not toxic.\n",
    "### Test 8: toxicity prob = 0.03 correctly determining this sentence is not toxic. \n",
    "### Test 9: toxicity prob = 0.96 incorrectly determining this sentence is toxic. \n",
    "### Test 10: toxicity prob = 0.13 correctly determining this sentence is not toxic. \n",
    "### Test 11: toxicity prob = 0.03 correctly determining this sentence is not toxic. \n",
    "### Test 12: toxicity prob = 0.39 correctly determining this sentence is not toxic. \n",
    "\n",
    "\n",
    "\n",
    "### After running these test results I have determined that my hypothesis was incorrect because on average the tests correctly determined the toxicity probabality of sentences with controversial terms. Furthermore, there were two tests that incorrectly determined the toxicity probability which tells us that there is a small bias.\n",
    "\n",
    "\n",
    "### In conclusion, there is a small bias within my hypothesis after running 12 tests. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a795c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
